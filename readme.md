## Опис проєкту

Проєкт: **Прогнозування підписання строкового депозиту**

Цей проєкт спрямований на побудову інтерпретованої та надійної моделі для прогнозування підписання клієнтом строкового
депозиту в банку на основі соціально-економічних та контактних характеристик.

Метою є не лише побудувати якісну модель, а й дослідити фактори, які впливають на рішення клієнта.

## Постановка задачі:

Задача бінарної класифікації: `y ∈ {yes, no}`. Ціль — передбачити, чи підпише клієнт депозит за результатами
маркетингової кампанії

Дані включають інформацію про:

- особисті характеристики клієнта (age, job, marital, education, тощо)
- кредитну історію (housing, loan, default)
- контактну інформацію та результати попередніх маркетингових кампаній (contact, month, pdays, campaign, previous,
  poutcome)
- соціально-економічні фактори (emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed)

Згідно з технічним завданням, у рамках проєкту потрібно було:

- Провести EDA (Exploratory Data Analysis)
- Обґрунтувати вибір метрик і методів побудови моделей
- Побудувати 4 різні типи моделей, включаючи:
- Logistic Regression
- k-Nearest Neighbors (kNN)
- Decision Tree
- Boosting (XGBoost / LightGBM / CatBoost)
- Застосувати тюнінг гіперпараметрів через RandomizedSearchCV та Hyperopt
- Провести SHAP-аналіз для найкращої моделі
- Проаналізувати помилкові передбачення та сформулювати шляхи покращення моделі

## Рішення

### Загальний опис

Проєкт містить у собі 2 основних ноутбуки:

- EDA: [notebooks/eda.ipynb](notebooks/eda.ipynb)
- Modeling: [notebooks/modeling.ipynb](notebooks/modeling.ipynb)

Обґрунтування обраних методів, метрик і стратегій: [docs/methodology.md](docs/methodology.md)

Результати моделювання: [docs/model_results_table.md](docs/model_results_table.md)

Моделювання реалізовано у вигляді модульного фреймворку, що дозволяє повторно використовувати код, масштабувати рішення
та проводити гнучкі експерименти.

### Виконано в ході проведення дослідження

#### EDA

Проведено аналіз, який дозволив виділити ключові ознаки та сформувати основні гіпотези.
Більш детально з дослідженням та отриманими висновками можна ознайомитись в [ноутбуці](notebooks/eda.ipynb).

#### **Preprocessing**

- Виконано обробку викидів
- Проведено кодування категоріальних колонок
- Застосовано масштабування ознак
- Зроблено видалення непотрібних ознак
- Створено додатково ознаки 'campaign_previous_ratio', 'second_half_year', 'weekday_call', 'total_contacts',
  'was_previously_contacted', 'campaign_to_total_ratio'

Усі налаштування контролюються через config.py, що забезпечує:

- гнучкість при експериментах
- прозору документацію підходів
- можливість легко застосувати розроблене рішення до інших задач

Такий підхід дозволив ефективно реалізувати препроцесинг, уникнути дублювання коду та зробити експерименти надзвичайно
зручними.

#### **Resampling**

Застосовано обробку дисбалансу класів з використанням SMOTENC.

Також розроблене рішення підтримує можливість використання ADASYN, але для дослідження цей метод не використовувався.

##### Візуалізація розподілу до ресемплінгу

До застосування будь-яких технік, було побудовано графік, який відображає
розподіл класів у просторі ознак після зниження розмірності PCA, що дозволяє візуально оцінити скупчення класів та
побачити ефект балансування.

#### **Modeling**

Загалом натреновано 24 моделі, серед яких:

- LogisticRegression
    - Baseline model
    - З додаванням поліноміальних ознак
    - Моделювання на даних після балансування класів
- KNeighborsClassifier (k=3, 5, 10)
    - Baseline model
    - Моделювання на даних після балансування класів
- DecisionTreeClassifier
    - Baseline model
    - Модель з налаштуванням гіперпараметрів вручну
    - Моделювання на даних після балансування класів з налаштуванням гіперпараметрів вручну
- RandomForestClassifier
    - Baseline model
    - Моделювання на даних після балансування класів
- XGBClassifier
    - Baseline model
    - Модель з налаштуванням гіперпараметрів за допомогою RandomizedSearchCV
    - Моделювання на даних після балансування класів з налаштуванням гіперпараметрів за допомогою RandomizedSearchCV
    - Модель з налаштуванням гіперпараметрів за допомогою Hyperopt
    - Моделювання на даних після балансування класів з налаштуванням гіперпараметрів за допомогою Hyperopt
- LGBMClassifier
    - Baseline model
    - Модель з налаштуванням гіперпараметрів за допомогою RandomizedSearchCV
    - Моделювання на даних після балансування класів з налаштуванням гіперпараметрів за допомогою RandomizedSearchCV
    - Модель з налаштуванням гіперпараметрів за допомогою Hyperopt
    - Моделювання на даних після балансування класів з налаштуванням гіперпараметрів за допомогою Hyperopt

Для деяких моделей(DecisionTreeClassifier, RandomForestClassifier) було застосовано крос валідацію для перевірки
стабільності моделі та оцінки її узагальнювальної здатності.
Реалізується як опціональна функція в BaseModel, яка дозволяє легко включати/виключати CV для кожної моделі.

Проведено порівняння якості моделей. Для кожної моделі застосовувалась як основна метрика - roc_auc, та як другорядна -
f1_score. Результуюче порівняння для визначення найкращої моделі виконано по основній метриці.
Для розгляду у випадку кожної окремої моделі застосовувалася і допоміжна метрика та відображення матриці плутанини для
формування більш об'єктивного коментаря про висновок щодо якості моделі.

Загалом розроблене рішення дає можливість додатково розрахувати й інші метрики, що налаштовується у фалі конфігурації,
та побудувати додаткові візуалізації.

#### **Налаштування через конфігураційні файли**

Рішення підтримує гнучке налаштування за допомогою одного централізованого файлу конфігурацій config.py. Це дозволяє
швидко змінювати логіку обробки даних, генерації ознак або вибору метрик без необхідності змінювати основний код.
Файл конфігурації: [config.py](utils/config.py)

##### Основні блоки налаштувань:

- PREPROCESSING_CONFIG: Увімкнення / вимкнення обробки unknown-значень, викидів, масштабування, feature engineering
- ORDINAL_MAPPINGS: Визначення порядку для OrdinalEncoder (наприклад, для освіти)
- ONE_HOT_COLS: Які категоріальні змінні кодувати за допомогою OneHotEncoder
- CATEGORY_MAPPINGS: Мапінг порядкових змінних (наприклад, місяць та день тижня)
- OUTLIER_CONFIG: Обробка викидів — обрізка, логарифмування, перетворення pdays
- MISSING_VALUE_CONFIG: Стратегії заповнення пропущених значень для кожної змінної
- FEATURE_ENGINEERING_CONFIG: Опис створюваних ознак (часові, контактні, співвідношення, тощо)
- METRIC_CONFIG: Основні та другорядні метрики, які використовуються під час оцінки моделей

Цей підхід забезпечує гнучкість, повторюваність і контрольованість усіх етапів побудови моделі.

#### **Аналіз моделі**

Для моделі, що показала себе найкраще - 'XGBClassifier з налаштуванням гіперпараметрів за допомогою RandomizedSearchCV'
було проведено аналіз ознак та аналіз помилок. Для цього було застосовано як і невеликий власно створений аналізатор,
так і бібліотека SHAP.

#### Висновки
Висновки після проведення дослідницького аналізу лани - описано в ноутбуці з [EDA](notebooks/eda.ipynb).
Всі висновки до кожної з розглянутих моделей, загальні висновки щодо проведених експериментів та важливості ознак
описані в ноутбуці [Modeling](notebooks/modeling.ipynb). 
Також сформовані припущення на основі проведеного аналізу, що можуть бути враховані банком при проведенні маркетингової
кампанії.